INFO 07-30 02:59:40 [__init__.py:239] Automatically detected platform cuda.
Using GPU 0
Loading model: /disk2/models/Llama-3.1-70B
WARNING 07-30 02:59:41 [config.py:2668] Casting torch.bfloat16 to torch.float16.
INFO 07-30 02:59:48 [config.py:600] This model supports multiple tasks: {'generate', 'score', 'reward', 'classify', 'embed'}. Defaulting to 'generate'.
WARNING 07-30 02:59:48 [config.py:679] dualfp quantization is not fully optimized yet. The speed can be slower than non-quantized models.
INFO 07-30 02:59:49 [config.py:1566] Defaulting to use ray for distributed inference
INFO 07-30 02:59:49 [config.py:1745] Chunked prefill is enabled with max_num_batched_tokens=16384.
WARNING 07-30 02:59:49 [cuda.py:96] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 07-30 02:59:50 [core.py:60] Initializing a V1 LLM engine (v0.8.3.dev133+g70fedd0f7) with config: model='/disk2/models/Llama-3.1-70B', speculative_config=None, tokenizer='/disk2/models/Llama-3.1-70B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=dualfp, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/disk2/models/Llama-3.1-70B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
INFO 07-30 02:59:51 [ray_utils.py:335] No current placement group found. Creating a new placement group.
INFO 07-30 02:59:51 [ray_distributed_executor.py:176] use_ray_spmd_worker: True
[36m(pid=407982)[0m INFO 07-30 02:59:55 [__init__.py:239] Automatically detected platform cuda.
INFO 07-30 02:59:57 [ray_distributed_executor.py:352] non_carry_over_env_vars from config: set()
INFO 07-30 02:59:57 [ray_distributed_executor.py:354] Copying the following environment variables to workers: ['LD_LIBRARY_PATH', 'VLLM_USE_RAY_SPMD_WORKER', 'VLLM_USE_RAY_COMPILED_DAG', 'VLLM_USE_V1']
INFO 07-30 02:59:57 [ray_distributed_executor.py:357] If certain env vars should NOT be copied to workers, add them to /home/ubuntu/.config/vllm/ray_non_carry_over_env_vars.json file
[36m(RayWorkerWrapper pid=407982)[0m WARNING 07-30 02:59:57 [utils.py:631] Overwriting environment variable LD_LIBRARY_PATH from '/opt/amazon/efa/lib:/opt/amazon/openmpi/lib:/opt/amazon/ofi-nccl/lib:/usr/local/cuda-12.6/lib:/usr/local/cuda-12.6/lib64:/usr/local/cuda-12.6:/usr/local/cuda-12.6/targets/x86_64-linux/lib/:/usr/local/cuda-12.6/extras/CUPTI/lib64:/usr/local/lib:/usr/lib:/opt/amazon/efa/lib:/opt/amazon/openmpi/lib:/opt/amazon/ofi-nccl/lib:/usr/local/cuda-12.6/lib:/usr/local/cuda-12.6/lib64:/usr/local/cuda-12.6:/usr/local/cuda-12.6/targets/x86_64-linux/lib/:/usr/local/cuda-12.6/extras/CUPTI/lib64:/usr/local/lib:/usr/lib' to '/opt/amazon/efa/lib:/opt/amazon/openmpi/lib:/opt/amazon/ofi-nccl/lib:/usr/local/cuda-12.6/lib:/usr/local/cuda-12.6/lib64:/usr/local/cuda-12.6:/usr/local/cuda-12.6/targets/x86_64-linux/lib/:/usr/local/cuda-12.6/extras/CUPTI/lib64:/usr/local/lib:/usr/lib:/usr/local/cuda/lib64:/opt/amazon/efa/lib:/opt/amazon/openmpi/lib:/opt/amazon/ofi-nccl/lib:/usr/local/cuda-12.6/lib:/usr/local/cuda-12.6/lib64:/usr/local/cuda-12.6:/usr/local/cuda-12.6/targets/x86_64-linux/lib/:/usr/local/cuda-12.6/extras/CUPTI/lib64:/usr/local/lib:/usr/lib:/opt/amazon/efa/lib:/opt/amazon/openmpi/lib:/opt/amazon/ofi-nccl/lib:/usr/local/cuda-12.6/lib:/usr/local/cuda-12.6/lib64:/usr/local/cuda-12.6:/usr/local/cuda-12.6/targets/x86_64-linux/lib/:/usr/local/cuda-12.6/extras/CUPTI/lib64:/usr/local/lib:/usr/lib'
[36m(RayWorkerWrapper pid=408145)[0m WARNING 07-30 02:59:59 [utils.py:2379] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7689156f7f90>
[36m(RayWorkerWrapper pid=407982)[0m INFO 07-30 03:00:02 [utils.py:982] Found nccl from library libnccl.so.2
[36m(RayWorkerWrapper pid=407982)[0m INFO 07-30 03:00:02 [pynccl.py:69] vLLM is using nccl==2.21.5
[36m(pid=408146)[0m INFO 07-30 02:59:56 [__init__.py:239] Automatically detected platform cuda.[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(RayWorkerWrapper pid=407982)[0m INFO 07-30 03:00:03 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
[36m(RayWorkerWrapper pid=407982)[0m INFO 07-30 03:00:03 [shm_broadcast.py:264] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_2de47f87'), local_subscribe_addr='ipc:///tmp/3f0d5261-508e-4426-b5ee-7a66fdb4b38d', remote_subscribe_addr=None, remote_addr_ipv6=False)
[36m(RayWorkerWrapper pid=408146)[0m WARNING 07-30 02:59:57 [utils.py:631] Overwriting environment variable LD_LIBRARY_PATH from '/opt/amazon/efa/lib:/opt/amazon/openmpi/lib:/opt/amazon/ofi-nccl/lib:/usr/local/cuda-12.6/lib:/usr/local/cuda-12.6/lib64:/usr/local/cuda-12.6:/usr/local/cuda-12.6/targets/x86_64-linux/lib/:/usr/local/cuda-12.6/extras/CUPTI/lib64:/usr/local/lib:/usr/lib:/opt/amazon/efa/lib:/opt/amazon/openmpi/lib:/opt/amazon/ofi-nccl/lib:/usr/local/cuda-12.6/lib:/usr/local/cuda-12.6/lib64:/usr/local/cuda-12.6:/usr/local/cuda-12.6/targets/x86_64-linux/lib/:/usr/local/cuda-12.6/extras/CUPTI/lib64:/usr/local/lib:/usr/lib' to '/opt/amazon/efa/lib:/opt/amazon/openmpi/lib:/opt/amazon/ofi-nccl/lib:/usr/local/cuda-12.6/lib:/usr/local/cuda-12.6/lib64:/usr/local/cuda-12.6:/usr/local/cuda-12.6/targets/x86_64-linux/lib/:/usr/local/cuda-12.6/extras/CUPTI/lib64:/usr/local/lib:/usr/lib:/usr/local/cuda/lib64:/opt/amazon/efa/lib:/opt/amazon/openmpi/lib:/opt/amazon/ofi-nccl/lib:/usr/local/cuda-12.6/lib:/usr/local/cuda-12.6/lib64:/usr/local/cuda-12.6:/usr/local/cuda-12.6/targets/x86_64-linux/lib/:/usr/local/cuda-12.6/extras/CUPTI/lib64:/usr/local/lib:/usr/lib:/opt/amazon/efa/lib:/opt/amazon/openmpi/lib:/opt/amazon/ofi-nccl/lib:/usr/local/cuda-12.6/lib:/usr/local/cuda-12.6/lib64:/usr/local/cuda-12.6:/usr/local/cuda-12.6/targets/x86_64-linux/lib/:/usr/local/cuda-12.6/extras/CUPTI/lib64:/usr/local/lib:/usr/lib'[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=408147)[0m INFO 07-30 03:00:03 [parallel_state.py:954] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3
[36m(RayWorkerWrapper pid=408147)[0m INFO 07-30 03:00:03 [cuda.py:221] Using Flash Attention backend on V1 engine.
[36m(RayWorkerWrapper pid=407982)[0m INFO 07-30 03:00:04 [gpu_model_runner.py:1186] Starting to load model /disk2/models/Llama-3.1-70B...
[36m(RayWorkerWrapper pid=407982)[0m WARNING 07-30 03:00:04 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[36m(RayWorkerWrapper pid=408146)[0m WARNING 07-30 02:59:59 [utils.py:2379] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7e00b96faed0>[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=407982)[0m Weight successfully split into upper and lower parts.
[36m(RayWorkerWrapper pid=408146)[0m INFO 07-30 03:00:02 [utils.py:982] Found nccl from library libnccl.so.2[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=408146)[0m INFO 07-30 03:00:02 [pynccl.py:69] vLLM is using nccl==2.21.5[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=408146)[0m INFO 07-30 03:00:03 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=408145)[0m INFO 07-30 03:00:03 [parallel_state.py:954] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=408145)[0m INFO 07-30 03:00:03 [cuda.py:221] Using Flash Attention backend on V1 engine.[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=408146)[0m INFO 07-30 03:00:04 [gpu_model_runner.py:1186] Starting to load model /disk2/models/Llama-3.1-70B...[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=408146)[0m WARNING 07-30 03:00:04 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=407982)[0m Weight successfully split into upper and lower parts.[32m [repeated 176x across cluster][0m
[36m(RayWorkerWrapper pid=408145)[0m Weight exceeds threshold, not splitting.
[36m(RayWorkerWrapper pid=408145)[0m Weight successfully split into upper and lower parts.[32m [repeated 175x across cluster][0m
[36m(RayWorkerWrapper pid=407982)[0m Weight exceeds threshold, not splitting.[32m [repeated 13x across cluster][0m
[36m(RayWorkerWrapper pid=407982)[0m Weight successfully split into upper and lower parts.[32m [repeated 194x across cluster][0m
[36m(RayWorkerWrapper pid=408145)[0m Weight exceeds threshold, not splitting.
[36m(RayWorkerWrapper pid=407982)[0m Weight successfully split into upper and lower parts.[32m [repeated 166x across cluster][0m
[36m(RayWorkerWrapper pid=407982)[0m Weight exceeds threshold, not splitting.[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=407982)[0m Weight successfully split into upper and lower parts.[32m [repeated 167x across cluster][0m
[36m(RayWorkerWrapper pid=408146)[0m Weight exceeds threshold, not splitting.[32m [repeated 25x across cluster][0m
[36m(RayWorkerWrapper pid=408147)[0m Weight successfully split into upper and lower parts.[32m [repeated 201x across cluster][0m
[36m(RayWorkerWrapper pid=407982)[0m Weight exceeds threshold, not splitting.
[36m(RayWorkerWrapper pid=407982)[0m INFO 07-30 03:00:39 [loader.py:447] Loading weights took 35.03 seconds
[36m(RayWorkerWrapper pid=408146)[0m INFO 07-30 03:00:39 [gpu_model_runner.py:1198] Model loading took 32.8894 GB and 35.359908 seconds
INFO 07-30 03:00:43 [kv_cache_utils.py:577] GPU KV cache size: 348,960 tokens
INFO 07-30 03:00:43 [kv_cache_utils.py:580] Maximum concurrency for 131,072 tokens per request: 2.66x
INFO 07-30 03:00:43 [kv_cache_utils.py:577] GPU KV cache size: 348,240 tokens
INFO 07-30 03:00:43 [kv_cache_utils.py:580] Maximum concurrency for 131,072 tokens per request: 2.66x
INFO 07-30 03:00:43 [kv_cache_utils.py:577] GPU KV cache size: 347,328 tokens
INFO 07-30 03:00:43 [kv_cache_utils.py:580] Maximum concurrency for 131,072 tokens per request: 2.65x
INFO 07-30 03:00:43 [kv_cache_utils.py:577] GPU KV cache size: 354,384 tokens
INFO 07-30 03:00:43 [kv_cache_utils.py:580] Maximum concurrency for 131,072 tokens per request: 2.70x
INFO 07-30 03:00:43 [core.py:159] init engine (profile, create kv cache, warmup model) took 3.51 seconds
Processing prompt: Explain the future of artifici...
INFO 07-30 03:00:43 [ray_distributed_executor.py:560] VLLM_USE_RAY_COMPILED_DAG_NCCL_CHANNEL = True
INFO 07-30 03:00:43 [ray_distributed_executor.py:562] VLLM_USE_RAY_COMPILED_DAG_OVERLAP_COMM = False
INFO 07-30 03:00:43 [ray_distributed_executor.py:570] RAY_CGRAPH_get_timeout is set to 300
Generation complete: Input tokens 9, Output tokens 256, Time: 33.9508s
Response:  The Future of Artificial Intelligence: The Next Frontier 2022-10-28
Artificial intelligence (AI) ha...

Processing prompt: What are the most famous tradi...
Generation complete: Input tokens 12, Output tokens 256, Time: 33.4003s
Response:  What is the best way to enjoy the delicious cuisine in Korea? In this article, weâ€™ll introduce you ...

Processing prompt: Why is Python popular among pr...
Generation complete: Input tokens 9, Output tokens 256, Time: 33.4377s
Response:  The answer to this question is that Python is an easy-to-use and powerful language. It has a clean ...

Processing prompt: Explain the impact of climate ...
Generation complete: Input tokens 11, Output tokens 30, Time: 3.9219s
Response:  Describe how human activities impact the Earth's climate. Explain how human activities impact the E...

Processing prompt: What are the basic principles ...
INFO 07-30 03:03:01 [ray_distributed_executor.py:127] Shutting down Ray distributed executor. If you see error log from logging.cc regarding SIGTERM received, please ignore because this is the expected termination process in Ray.
Generation complete: Input tokens 10, Output tokens 256, Time: 33.3989s
Response:  What is a qubit? What is the difference between a qubit and a classical bit? What is superposition ...


Test Summary:
Number of prompts tested: 5
Average generation time: 27.6219s
Average output tokens: 210.8
Results saved to: test_results.csv
