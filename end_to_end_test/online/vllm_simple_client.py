import asyncio
import argparse
import pandas as pd
import numpy as np
import time
from typing import Optional
from datetime import datetime
import json
import os
import sys
import traceback
import re
from dataclasses import dataclass, field
import aiohttp
from transformers import AutoTokenizer
import random
import matplotlib.pyplot as plt

@dataclass
class RequestFuncInput:
    prompt: str
    api_url: str
    prompt_len: int
    output_len: int
    model: str
    model_name: Optional[str] = None

@dataclass
class RequestFuncOutput:
    generated_text: str = ""
    success: bool = False
    latency: float = 0.0
    ttft: float = 0.0
    itl: list[float] = field(default_factory=list)
    tpot: float = 0.0
    prompt_len: int = 0
    error: str = ""
    token_arrival_times: list[float] = field(default_factory=list)
    request_sent_time: float = 0.0
    request_completed_time: float = 0.0
    # ìƒˆë¡œ ì¶”ê°€: iteration ì •ë³´
    iteration_data: list[dict] = field(default_factory=list)

# ì „ì—­ ë³€ìˆ˜ë“¤
ttft_graph = {'iteration_step': [], 'ttft': []}
iter_tpot_graph = {}  # {iteration_total: [token_latencies]}
iter_kv_graph = {}  # {iteration_total: [kv_cache_usage]}
iter_kv_gb_graph = {}  # {iteration_total: [kv_cache_usage_gb]}
iter_kv_total_capacity_graph = {}  # {iteration_total: [kv_cache_total_capacity]}
iter_num_prefill_graph = {}  # {iteration_total: [num_prefill]}
iter_num_decode_graph = {}  # {iteration_total: [num_decode]}

# ğŸ†• iterationë³„ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ìš© ì „ì—­ ë³€ìˆ˜
iteration_details = {}  # {iteration_total: {...}}
iteration_lock = asyncio.Lock()  # async í™˜ê²½ì—ì„œ ì•ˆì „í•œ ì ‘ê·¼ì„ ìœ„í•œ ë½

async def async_request_openai_completions(
    request_func_input: RequestFuncInput,
) -> RequestFuncOutput:
    api_url = request_func_input.api_url
    assert api_url.endswith(("completions", "profile")), \
        "OpenAI Completions API URL must end with 'completions' or 'profile'."

    AIOHTTP_TIMEOUT = aiohttp.ClientTimeout(total=6 * 60 * 60)
    async with aiohttp.ClientSession(trust_env=True, timeout=AIOHTTP_TIMEOUT) as session:
        payload = {
            "model": request_func_input.model_name or request_func_input.model,
            "prompt": request_func_input.prompt,
            "temperature": 0.0,
            "repetition_penalty": 1.0,
            "max_tokens": request_func_input.output_len,
            "stream": True,
            "stream_options": {"include_usage": True},
            "ignore_eos": True,
        }
        
        headers = {}
        openai_api_key = os.environ.get('OPENAI_API_KEY')
        if openai_api_key:
            headers["Authorization"] = f"Bearer {openai_api_key}"

        output = RequestFuncOutput()
        output.prompt_len = request_func_input.prompt_len

        generated_text = ""
        st = time.perf_counter()
        output.request_sent_time = st
        previous_timestamp = None  # ğŸ†• ITL ê³„ì‚°ì„ ìœ„í•œ ì´ì „ timestamp ì¶”ì 
        
        try:
            async with session.post(url=api_url, json=payload, headers=headers) as response:
                if response.status == 200:
                    first_chunk_received = False
                    async for chunk_bytes in response.content:
                        chunk_bytes = chunk_bytes.strip()
                        if not chunk_bytes:
                            continue

                        chunk = chunk_bytes.decode("utf-8").removeprefix("data: ")
                        if chunk != "[DONE]":
                            data = json.loads(chunk)

                            if choices := data.get("choices"):
                                text = choices[0].get("text")
                                timestamp = choices[0].get("iteration_timestamp")
                                iteration_total = choices[0].get("iteration_total")
                                kv_cache_usage = choices[0].get("kv_cache_usage")
                                kv_cache_usage_gb = choices[0].get("kv_cache_usage_gb")
                                kv_cache_total_capacity = choices[0].get("kv_cache_total_capacity")
                                num_prefill = choices[0].get("num_prefill")
                                num_decode = choices[0].get("num_decode")
                                # ì„œë²„ì—ì„œ ì˜¨ ìƒˆë¡œìš´ ìŠ¤ì¼€ì¤„ë§ ì •ë³´ë“¤
                                total_scheduled_requests = choices[0].get("total_scheduled_requests")
                                total_scheduled_tokens = choices[0].get("total_scheduled_tokens")
                                prefill_requests = choices[0].get("prefill_requests") 
                                decode_requests = choices[0].get("decode_requests")
                                prefill_tokens = choices[0].get("prefill_tokens")
                                decode_tokens = choices[0].get("decode_tokens")
                                request_details = choices[0].get("request_details", [])
                                
                                # iteration ì •ë³´ ì €ì¥ (ê¸°ì¡´ - RequestFuncOutputìš©)
                                if timestamp is not None and iteration_total is not None:
                                    output.iteration_data.append({
                                        "iteration_total": iteration_total,
                                        "timestamp": timestamp,
                                        "kv_cache_usage": kv_cache_usage,
                                        "text": text or ""
                                    })
                                
                                # ğŸ†• ë‹¨ìˆœí™”ëœ iterationë³„ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
                                if iteration_total is not None:
                                    async with iteration_lock:
                                        if iteration_total not in iteration_details:
                                            iteration_details[iteration_total] = {
                                                "iteration_total": iteration_total,
                                                "timestamp": timestamp,
                                                "tokens_generated": 0,
                                                "kv_cache_usage": kv_cache_usage,
                                                "kv_cache_usage_gb": kv_cache_usage_gb,
                                                "kv_cache_total_capacity": kv_cache_total_capacity,
                                                "total_scheduled_requests": total_scheduled_requests,
                                                "total_scheduled_tokens": total_scheduled_tokens,
                                                "prefill_requests": prefill_requests,
                                                "decode_requests": decode_requests,
                                                "prefill_tokens": prefill_tokens,
                                                "decode_tokens": decode_tokens,
                                                "request_details": request_details,
                                                "itl": None,  # ğŸ†• ë‹¨ì¼ ITL ê°’
                                            }
                                        
                                        # ê¸°ì¡´ iterationì´ë©´ ìµœì‹  ì •ë³´ë¡œ ì—…ë°ì´íŠ¸
                                        iter_data = iteration_details[iteration_total]
                                        iter_data["timestamp"] = timestamp
                                        iter_data["tokens_generated"] += 1
                                        
                                        # ğŸ†• ITL ê³„ì‚° (ì´ì „ í† í°ê³¼ì˜ ì‹œê°„ì°¨)
                                        if previous_timestamp is not None:
                                            itl = timestamp - previous_timestamp
                                            iter_data["itl"] = itl
                                
                                output.token_arrival_times.append(timestamp)
                                
                                if not first_chunk_received:
                                    first_chunk_received = True
                                    ttft = timestamp - st
                                    print(f'{st} to {timestamp}: so ttft is {ttft}')
                                    output.ttft = ttft
                                    ttft_graph['iteration_step'].append(iteration_total)
                                    ttft_graph['ttft'].append(ttft)
                                else:
                                    output.itl.append(timestamp - previous_timestamp if previous_timestamp else 0)
                                    
                                # ğŸ”„ ê¸°ì¡´ ê·¸ë˜í”„ ë°ì´í„° ìˆ˜ì§‘ (í˜¸í™˜ì„± ìœ ì§€)
                                if iteration_total is not None and previous_timestamp is not None:
                                    token_latency = timestamp - previous_timestamp
                                    if iteration_total not in iter_tpot_graph:
                                        iter_tpot_graph[iteration_total] = []
                                    iter_tpot_graph[iteration_total].append(token_latency)
                                    
                                iter_kv_graph[iteration_total] = [kv_cache_usage]
                                iter_kv_gb_graph[iteration_total] = [kv_cache_usage_gb]
                                iter_kv_total_capacity_graph[iteration_total] = [kv_cache_total_capacity]
                                iter_num_prefill_graph[iteration_total] = [num_prefill]
                                iter_num_decode_graph[iteration_total] = [num_decode]

                                # ğŸ†• ì´ì „ timestamp ì—…ë°ì´íŠ¸
                                if timestamp is not None:
                                    previous_timestamp = timestamp
                                    
                                generated_text += text or ""
                    
                    output.request_completed_time = time.perf_counter()
                    
                    if first_chunk_received:
                        output.success = True
                        output.generated_text = generated_text
                        output.latency = output.request_completed_time - st
                        if output.itl:
                            output.tpot = sum(output.itl) / len(output.itl)
                    else:
                        output.success = False
                        output.error = "Never received a valid chunk to calculate TTFT."
                else:
                    output.error = response.reason or ""
                    output.success = False
        except Exception:
            output.success = False
            exc_info = sys.exc_info()
            output.error = "".join(traceback.format_exception(*exc_info))

        return output


# ğŸ†• iteration ìƒì„¸ ì •ë³´ í›„ì²˜ë¦¬ í•¨ìˆ˜
def process_iteration_details():
    """ìˆ˜ì§‘ëœ iteration ì •ë³´ë¥¼ í›„ì²˜ë¦¬í•˜ì—¬ ìµœì¢… í˜•íƒœë¡œ ë³€í™˜"""
    processed_iterations = []
    
    for iteration_total in sorted(iteration_details.keys()):
        iter_data = iteration_details[iteration_total]
        
        # ğŸ†• ë‹¨ìˆœí™”ëœ êµ¬ì¡°
        processed_iter = {
            "iteration_total": iteration_total,
            "timestamp": iter_data["timestamp"],  # ğŸ†• ë‹¨ì¼ timestamp
            "tokens_generated": iter_data["tokens_generated"],
            
            # ìŠ¤ì¼€ì¤„ë§ ì •ë³´
            "total_scheduled_requests": iter_data["total_scheduled_requests"],
            "total_scheduled_tokens": iter_data["total_scheduled_tokens"],
            "prefill_requests": iter_data["prefill_requests"], 
            "decode_requests": iter_data["decode_requests"],
            "prefill_tokens": iter_data["prefill_tokens"],
            "decode_tokens": iter_data["decode_tokens"],
            
            # KV cache ì •ë³´
            "kv_cache_usage": iter_data["kv_cache_usage"],
            "kv_cache_usage_gb": iter_data["kv_cache_usage_gb"],
            "kv_cache_total_capacity": iter_data["kv_cache_total_capacity"],
            # ğŸ†• ITL (Inter-Token Latency) - ë‹¨ì¼ ê°’
            "itl": iter_data["itl"],
            # ìš”ì²­ë³„ ì„¸ë¶€ ì •ë³´
            "request_details": iter_data["request_details"]
        }
        
        processed_iterations.append(processed_iter)
    
    return processed_iterations


# ê¸°ì¡´ í•¨ìˆ˜ë“¤ì€ ê·¸ëŒ€ë¡œ ìœ ì§€...
def pregenerate_prompts(trace_df, tokenizer_name="Qwen/Qwen2.5-7B"):
    """íš¨ìœ¨ì ìœ¼ë¡œ ì •í™•í•œ í† í° ê¸¸ì´ì˜ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
    print(f"Pre-generating prompts using tokenizer: {tokenizer_name}")
    start_time = time.time()

    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
   
    # ê°„ë‹¨í•œ ë‹¨ì–´ í’€
    words = ["hello", "world", "test", "data", "model", "training", "computer", "science",
            "artificial", "intelligence", "machine", "learning", "neural", "network",
            "transformer", "attention", "embedding", "layer", "parameter", "gradient",
            "system", "processing", "algorithm", "function", "method", "class", "object",
            "memory", "storage", "database", "server", "client", "protocol", "interface"]
   
    prompts = []
    
    for idx, row in trace_df.iterrows():
        target_tokens = row['CONTEXT_TOKENS']
        
        # requestë³„ ë‹¤ë¥¸ ì‹œë“œ ì‚¬ìš© (KV Cache ë‹¤ì–‘ì„± í™•ë³´)
        random.seed(idx)
        
        # ì¶©ë¶„íˆ ê¸´ í”„ë¡¬í”„íŠ¸ ìƒì„± (target_tokensì˜ 1.5ë°° ì •ë„)
        selected_words = random.choices(words, k=target_tokens * 2)
        prompt = " ".join(selected_words)
        
        # í† í° ê¸¸ì´ í™•ì¸ ë° ì¡°ì •
        encoded = tokenizer.encode(prompt, add_special_tokens=False)
        
        if len(encoded) > target_tokens:
            # í† í° ë‹¨ìœ„ë¡œ ì •í™•íˆ ìë¥´ê¸°
            truncated_tokens = encoded[:target_tokens]
            prompt = tokenizer.decode(truncated_tokens)
        elif len(encoded) < target_tokens:
            # ë¶€ì¡±í•œ ê²½ìš° ë‹¨ì–´ ë” ì¶”ê°€
            while len(encoded) < target_tokens:
                additional_word = random.choice(words)
                test_prompt = prompt + " " + additional_word
                test_encoded = tokenizer.encode(test_prompt, add_special_tokens=False)
                if len(test_encoded) <= target_tokens:
                    prompt = test_prompt
                    encoded = test_encoded
                else:
                    break
        
        prompts.append(prompt)
        
        if (idx + 1) % 100 == 0:
            print(f"Generated {idx + 1}/{len(trace_df)} prompts")
    
    # ê²€ì¦
    print("\nValidating generated prompts...")
    mismatches = 0
    for i, (prompt, target_length) in enumerate(zip(prompts, trace_df['CONTEXT_TOKENS'])):
        actual_length = len(tokenizer.encode(prompt, add_special_tokens=False))
        if actual_length != target_length:
            mismatches += 1
            if mismatches <= 5:
                print(f"Mismatch at index {i}: target={target_length}, actual={actual_length}")
    
    generation_time = time.time() - start_time
    print(f"Prompt generation completed in {generation_time:.2f}s")
    print(f"Token length mismatches: {mismatches}/{len(prompts)}")
    
    return prompts

def load_trace_data(file_path, num_requests=None, duration_minutes=None):
    """
    Trace ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  í•„í„°ë§
    
    Args:
        file_path: CSV íŒŒì¼ ê²½ë¡œ
        num_requests: ì‚¬ìš©í•  ìš”ì²­ ê°œìˆ˜ (legacy, duration_minutesì™€ í•¨ê»˜ ì‚¬ìš© ë¶ˆê°€)
        duration_minutes: ì‹¤í—˜ ì§€ì† ì‹œê°„ (ë¶„ ë‹¨ìœ„)
    """
    df = pd.read_csv(file_path)
    df['TIMESTAMP'] = pd.to_datetime(df.iloc[:, 0])
    df.columns = ['TIMESTAMP', 'CONTEXT_TOKENS', 'GENERATED_TOKENS']
    
    # ìƒëŒ€ ì‹œê°„ ê³„ì‚°
    first_timestamp = df['TIMESTAMP'].min()
    df['relative_time'] = (df['TIMESTAMP'] - first_timestamp).dt.total_seconds()
    
    # trace ë°ì´í„°ì˜ ì´ ì§€ì† ì‹œê°„ ê³„ì‚°
    total_duration_seconds = df['relative_time'].max()
    total_duration_minutes = total_duration_seconds / 60
    
    print(f"Trace file loaded: {len(df)} total requests")
    print(f"Trace duration: {total_duration_minutes:.2f} minutes ({total_duration_seconds:.2f} seconds)")
    
    # ë‘ íŒŒë¼ë¯¸í„°ê°€ ëª¨ë‘ ì œê³µëœ ê²½ìš° ì—ëŸ¬
    if num_requests is not None and duration_minutes is not None:
        raise ValueError("Cannot specify both num_requests and duration_minutes. Please use only one.")
    
    # duration_minutes ê¸°ì¤€ìœ¼ë¡œ í•„í„°ë§
    if duration_minutes is not None:
        target_duration_seconds = duration_minutes * 60
        
        # ìš”ì²­í•œ ì‹œê°„ì´ trace ë°ì´í„°ë³´ë‹¤ ê¸´ ê²½ìš° ì—ëŸ¬
        if target_duration_seconds > total_duration_seconds:
            raise ValueError(f"Requested duration ({duration_minutes} minutes) exceeds trace data duration ({total_duration_minutes:.2f} minutes)")
        
        # ì§€ì •ëœ ì‹œê°„ ë‚´ì˜ ìš”ì²­ë§Œ í•„í„°ë§
        filtered_df = df[df['relative_time'] <= target_duration_seconds].copy()
        
        print(f"Using {duration_minutes} minutes of trace data: {len(filtered_df)} requests")
        print(f"Actual duration used: {filtered_df['relative_time'].max():.2f} seconds")
        
        return filtered_df
    
    # num_requests ê¸°ì¤€ìœ¼ë¡œ í•„í„°ë§ (legacy ì§€ì›)
    elif num_requests is not None:
        if len(df) > num_requests:
            df = df.head(num_requests)
            print(f"Using first {num_requests} requests from trace file")
            print(f"Duration of selected requests: {df['relative_time'].max():.2f} seconds ({df['relative_time'].max()/60:.2f} minutes)")
        else:
            print(f"Using all {len(df)} requests from trace file")
        
        return df
    
    # ì•„ë¬´ê²ƒë„ ì§€ì •ë˜ì§€ ì•Šì€ ê²½ìš° ëª¨ë“  ë°ì´í„° ì‚¬ìš©
    else:
        print(f"Using all {len(df)} requests from trace file")
        return df

async def execute_single_request_with_prompt(request_input, prompt, api_url, model_name, request_id):
    """ë¯¸ë¦¬ ìƒì„±ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•˜ëŠ” ë²„ì „"""
    input_obj = RequestFuncInput(
        prompt=prompt,  # ë¯¸ë¦¬ ìƒì„±ëœ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
        api_url=api_url,
        prompt_len=request_input['CONTEXT_TOKENS'],
        output_len=request_input['GENERATED_TOKENS'],
        model=model_name
    )
    
    result = await async_request_openai_completions(input_obj)
    return result, request_id

async def execute_trace_based_requests(trace_df, prompts, api_url, model_name):
    results = []
    start_time = time.perf_counter()
    
    print(f"Starting trace-based test with {len(trace_df)} requests")
    
    for idx, row in trace_df.iterrows():
        target_time = row['relative_time']
        
        # ì •í™•í•œ ì‹œê°„ê¹Œì§€ ëŒ€ê¸°
        while True:
            current_time = time.perf_counter() - start_time
            if current_time >= target_time:
                break
            await asyncio.sleep(0.001)  # 1ms ê°„ê²©ìœ¼ë¡œ ì²´í¬
        
        # ìš”ì²­ ì „ì†¡
        task = asyncio.create_task(
            execute_single_request_with_prompt(row, prompts[idx], api_url, model_name, idx)
        )
        results.append((task, row, idx))
        
        send_time = time.perf_counter() - start_time
        print(f"Request {idx+1}/{len(trace_df)} sent at {send_time:.6f}s (target: {target_time:.6f}s)")
    
    print(f"All requests sent. Waiting for responses...")
    
    final_results = []
    for task, row, request_id in results:
        result, _ = await task
        
        actual_generated_tokens = len(result.itl) + 1 if result.success and result.itl else 0
        
        final_results.append({
            'request_id': request_id,
            'context_tokens': row['CONTEXT_TOKENS'],
            'generated_tokens': row['GENERATED_TOKENS'],
            'actual_generated_tokens': actual_generated_tokens,
            'success': result.success,
            'latency': result.latency,
            'ttft': result.ttft,
            'tpot': result.tpot,
            'error': result.error,
            'request_sent_time': result.request_sent_time,
            'request_completed_time': result.request_completed_time,
            'token_arrival_times': result.token_arrival_times,
            'itl': result.itl,
            'iteration_data': result.iteration_data
        })
    
    return final_results

# ê¸°ì¡´ ê·¸ë˜í”„ ìƒì„± í•¨ìˆ˜ë“¤ì€ ê·¸ëŒ€ë¡œ ìœ ì§€...
def save_ttft_plot(dic, x, y, file_name):
    df = pd.DataFrame(dic)
    if len(df) > 0:
        df = df.sort_values(x)
        plt.figure(figsize=(10, 6))
        plt.plot(df[x], df[y], '-', linewidth=1, alpha=0.6, color='lightblue')
        plt.scatter(df[x], df[y], s=50, alpha=0.9, color='red', edgecolors='darkred', linewidth=1)
        plt.xlabel('First Token Iteration Step')  # ì˜ë¯¸ìˆëŠ” ë¼ë²¨
        plt.ylabel('TTFT [s]')
        plt.title('TTFT by First Token Iteration Step')
        plt.grid(True, alpha=0.3)
        plt.savefig(f'{file_name}.png', dpi=1000)
        plt.close()

def save_num_data_plots(iter_dict, file_name, data_name):
    """ì„œë²„ iteration stepë³„ num_prefill or num_decode ê·¸ë˜í”„ ì €ì¥"""
    if not iter_dict:
        print(f"No {data_name} data to plot")
        return
    
    iterations = []
    latencies = []
    
    for iteration_step in sorted(iter_dict.keys()):
        step_latencies = iter_dict[iteration_step]
        for latency in step_latencies:
            iterations.append(iteration_step)
            latencies.append(latency)
    
    if not iterations:
        print(f"No valid {data_name} data to plot")
        return
    
    # ë‹¨ì¼ ê·¸ë˜í”„ ìƒì„± (ì„ ìœ¼ë¡œ ì—°ê²°)
    plt.figure(figsize=(12, 6))
    plt.plot(iterations, latencies, 'o-', markersize=4, linewidth=1, alpha=0.7, color='blue')
    plt.xlabel('Server Iteration Step')
    plt.ylabel(f'{data_name} [number]')
    plt.title(f'{data_name} by Iteration Step')
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(f'{file_name}.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"{data_name} saved to {file_name}.png")
    
    # ê°„ë‹¨í•œ í†µê³„ ì •ë³´ ì¶œë ¥
    print(f"\n{data_name} usage Statistics:")
    print(f"Total iteration steps executed: {len(iter_dict)}")  # ì‹¤ì œ ì‹¤í–‰ëœ iteration step íšŸìˆ˜
    print(f"Unique iteration step range: {min(iterations)} - {max(iterations)}")
    if latencies:
        print(f"Average {data_name} usage: {np.mean(latencies):.4f}s")
        print(f"Min {data_name} usage: {min(latencies):.4f}s")
        print(f"Max {data_name} usage: {max(latencies):.4f}s")

def save_kv_plots(iter_dict, file_name):
    """ì„œë²„ iteration stepë³„ kv_cache_usage ê·¸ë˜í”„ ì €ì¥"""
    if not iter_dict:
        print("No kv usage data to plot")
        return
    
    iterations = []
    latencies = []
    
    for iteration_step in sorted(iter_dict.keys()):
        step_latencies = iter_dict[iteration_step]
        for latency in step_latencies:
            iterations.append(iteration_step)
            latencies.append(latency)
    
    if not iterations:
        print("No valid TPOT data to plot")
        return
    
    # ë‹¨ì¼ ê·¸ë˜í”„ ìƒì„± (ì„ ìœ¼ë¡œ ì—°ê²°)
    plt.figure(figsize=(12, 6))
    plt.plot(iterations, latencies, 'o-', markersize=4, linewidth=1, alpha=0.7, color='blue')
    plt.xlabel('Server Iteration Step')
    plt.ylabel('kv cache usage [ratio]')
    plt.title('kv cache usage by Iteration Step')
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(f'{file_name}.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"kv plot saved to {file_name}.png")
    
    # ê°„ë‹¨í•œ í†µê³„ ì •ë³´ ì¶œë ¥
    print(f"\nkv cache usage Statistics:")
    print(f"Total iteration steps executed: {len(iter_dict)}")  # ì‹¤ì œ ì‹¤í–‰ëœ iteration step íšŸìˆ˜
    print(f"Unique iteration step range: {min(iterations)} - {max(iterations)}")
    if latencies:
        print(f"Average kv cache usage: {np.mean(latencies):.4f}s")
        print(f"Min kv cache usage: {min(latencies):.4f}s")
        print(f"Max kv cache usage: {max(latencies):.4f}s")

def save_tpot_plots(iter_dict, file_name):
    """ì„œë²„ iteration stepë³„ token latency ê·¸ë˜í”„ ì €ì¥"""
    if not iter_dict:
        print("No TPOT data to plot")
        return
    
    # iteration stepê³¼ í•´ë‹¹ latency ë°ì´í„° ì¤€ë¹„
    iterations = []
    latencies = []
    
    for iteration_step in sorted(iter_dict.keys()):
        step_latencies = iter_dict[iteration_step]
        for latency in step_latencies:
            iterations.append(iteration_step)
            latencies.append(latency)
    
    if not iterations:
        print("No valid TPOT data to plot")
        return
    
    # ë‹¨ì¼ ê·¸ë˜í”„ ìƒì„± (ì„ ìœ¼ë¡œ ì—°ê²°)
    plt.figure(figsize=(12, 6))
    plt.plot(iterations, latencies, 'o-', markersize=4, linewidth=1, alpha=0.7, color='blue')
    plt.xlabel('Server Iteration Step')
    plt.ylabel('Token Latency [s]')
    plt.title('Token Latency by Iteration Step')
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(f'{file_name}.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"TPOT plot saved to {file_name}.png")
    
    # ê°„ë‹¨í•œ í†µê³„ ì •ë³´ ì¶œì¶œ
    print(f"\nTPOT Statistics:")
    print(f"Total iteration steps executed: {len(iter_dict)}")  # ì‹¤ì œ ì‹¤í–‰ëœ iteration step íšŸìˆ˜
    print(f"Unique iteration step range: {min(iterations)} - {max(iterations)}")
    print(f"Total tokens processed: {len(latencies)}")  # ìƒì„±ëœ ì´ í† í° ê°œìˆ˜
    if latencies:
        print(f"Average token latency: {np.mean(latencies):.4f}s")
        print(f"Min token latency: {min(latencies):.4f}s")
        print(f"Max token latency: {max(latencies):.4f}s")

def create_performance_scatter_plots(results, middle_ratio=0.8):
    """
    TTFT vs TPOT ì‚°í¬ë„ ìƒì„± ë° ì„±ëŠ¥ í†µê³„ ê³„ì‚°
    
    Args:
        results: ì‹¤í—˜ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        middle_ratio: ì¤‘ê°„ êµ¬ê°„ ë¹„ìœ¨ (0.8ì´ë©´ ì•ë’¤ 10%ì”© ì œê±°í•˜ê³  ì¤‘ê°„ 80% ì‚¬ìš©)
    """
    
    # ì„±ê³µí•œ ìš”ì²­ë“¤ì˜ ë°ì´í„°ë§Œ ì¶”ì¶œ (request_id ìˆœì„œë¡œ ì •ë ¬)
    successful_results = [r for r in results if r['success']]
    successful_results.sort(key=lambda x: x['request_id'])  # request_id ìˆœì„œë¡œ ì •ë ¬
    
    total_count = len(results)
    success_count = len(successful_results)
    
    print(f"Completed {success_count}/{total_count} requests successfully")
    
    if not successful_results:
        print("No successful results to plot or calculate stats")
        return {}, success_count
    
    # ì¤‘ê°„ êµ¬ê°„ ê³„ì‚°
    if middle_ratio >= 1.0 or middle_ratio <= 0:
        print(f"Warning: Invalid middle_ratio {middle_ratio}, using all data")
        filtered_results = successful_results
        start_idx = 0
        end_idx = len(successful_results)
    else:
        total_successful = len(successful_results)
        skip_count = int(total_successful * (1 - middle_ratio) / 2)
        start_idx = skip_count
        end_idx = total_successful - skip_count
        filtered_results = successful_results[start_idx:end_idx]
    
    print(f"Using middle {middle_ratio*100}% of data: requests {start_idx} to {end_idx-1} (total: {len(filtered_results)})")
    
    if not filtered_results:
        print("No data remaining after filtering")
        return {}, success_count
    
    # í•„í„°ë§ëœ ë°ì´í„°ì—ì„œ í†µê³„ ê³„ì‚°
    latency_values = [r['latency'] for r in filtered_results]
    ttft_values = [r['ttft'] for r in filtered_results if r['ttft'] > 0]
    tpot_values = [r['tpot'] for r in filtered_results if r['tpot'] > 0]
    
    # í†µê³„ ê³„ì‚°
    stats = {}
    stats['filtered_count'] = len(filtered_results)
    stats['filter_range'] = f"{start_idx}-{end_idx-1}"
    stats['avg_latency'] = np.mean(latency_values)
    
    if ttft_values:
        stats['avg_ttft'] = np.mean(ttft_values)
        stats['p90_ttft'] = np.percentile(ttft_values, 90)
        stats['p99_ttft'] = np.percentile(ttft_values, 99)
    
    if tpot_values:
        stats['avg_tpot'] = np.mean(tpot_values)
        stats['p90_tpot'] = np.percentile(tpot_values, 90)
        stats['p99_tpot'] = np.percentile(tpot_values, 99)
    
    # í†µê³„ ì¶œë ¥
    print(f"\n--- Performance Statistics (Middle {middle_ratio*100}% of requests) ---")
    print(f"Filtered data range: requests {start_idx} to {end_idx-1} ({len(filtered_results)} requests)")
    print(f"Average latency: {stats.get('avg_latency', 0):.4f}s")
    if 'avg_ttft' in stats:
        print(f"Average TTFT: {stats['avg_ttft']:.4f}s")
        print(f"P90 TTFT: {stats['p90_ttft']:.4f}s")
        print(f"P99 TTFT: {stats['p99_ttft']:.4f}s")
    if 'avg_tpot' in stats:
        print(f"Average TPOT: {stats['avg_tpot']:.4f}s")
        print(f"P90 TPOT: {stats['p90_tpot']:.4f}s")
        print(f"P99 TPOT: {stats['p99_tpot']:.4f}s")
    
    # ì‚°í¬ë„ë¥¼ ìœ„í•œ í˜ì–´ ë°ì´í„° (í•„í„°ë§ëœ ê²°ê³¼ë§Œ ì‚¬ìš©)
    paired_data = [(r['tpot'], r['ttft']) for r in filtered_results 
                   if r['ttft'] > 0 and r['tpot'] > 0]  # x=TPOT, y=TTFT
    
    if not paired_data:
        print("No paired TTFT/TPOT data to plot")
        return stats, success_count
    
    paired_tpot, paired_ttft = zip(*paired_data)
    
    # ì¶• ë²”ìœ„ ê³„ì‚° (ì—¬ìœ ìˆê²Œ)
    tpot_max = max(paired_tpot)
    ttft_max = max(paired_ttft)
    tpot_min = min(paired_tpot)
    ttft_min = min(paired_ttft)
    
    tpot_margin = (tpot_max - tpot_min) * 0.1  # 10% ì—¬ìœ 
    ttft_margin = (ttft_max - ttft_min) * 0.1  # 10% ì—¬ìœ 
    
    # ë‹¨ì¼ ì‚°í¬ë„ ìƒì„±
    plt.figure(figsize=(12, 8))  # ë²”ë¡€ê°€ ë“¤ì–´ê°ˆ ê³µê°„ì„ ìœ„í•´ ë„ˆë¹„ë¥¼ ì¡°ê¸ˆ ëŠ˜ë¦¼
    plt.scatter(paired_tpot, paired_ttft, alpha=0.7, s=50, color='blue', edgecolors='darkblue', linewidth=0.5)
    plt.xlabel('TPOT [s]')
    plt.ylabel('TTFT [s]')
    plt.title(f'TTFT vs TPOT Scatter Plot (Middle {middle_ratio*100}% of requests)')
    plt.grid(True, alpha=0.3)
    
    # ì¶• ë²”ìœ„ ì„¤ì • (ì—¬ìœ ìˆê²Œ)
    x_min, x_max = tpot_min - tpot_margin, tpot_max + tpot_margin
    y_min, y_max = ttft_min - ttft_margin, ttft_max + ttft_margin
    plt.xlim(x_min, x_max)
    plt.ylim(y_min, y_max)
    
    # í‰ê· ê°’ê³¼ P90 ê°’ ê³„ì‚°
    avg_tpot = np.mean(paired_tpot)
    avg_ttft = np.mean(paired_ttft)
    p90_tpot = np.percentile(paired_tpot, 90)
    p90_ttft = np.percentile(paired_ttft, 90)
    
    # í‰ê· ì„  ì¶”ê°€ (ì‹¤ì„ )
    plt.axvline(avg_tpot, color='red', linestyle='-', alpha=0.8, linewidth=2, 
                label=f'Avg TPOT: {avg_tpot:.3f}s')
    plt.axhline(avg_ttft, color='green', linestyle='-', alpha=0.8, linewidth=2, 
                label=f'Avg TTFT: {avg_ttft:.3f}s')
    
    # P90 ì„  ì¶”ê°€ (ì ì„ )
    plt.axvline(p90_tpot, color='red', linestyle='--', alpha=0.8, linewidth=2, 
                label=f'P90 TPOT: {p90_tpot:.3f}s')
    plt.axhline(p90_ttft, color='green', linestyle='--', alpha=0.8, linewidth=2, 
                label=f'P90 TTFT: {p90_ttft:.3f}s')
    
    # ë²”ë¡€ ìœ„ì¹˜ ì¡°ì • (ê·¸ë˜í”„ ë°–ìœ¼ë¡œ ë°°ì¹˜)
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.tight_layout()
    
    # íŒŒì¼ ì €ì¥
    scatter_file = f"performance_scatter_middle_{int(middle_ratio*100)}pct.png"
    plt.savefig(scatter_file, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"Performance scatter plot saved to {scatter_file}")
    
    # ì¶”ê°€ í†µê³„ ì •ë³´ ì¶œë ¥
    print(f"\nDetailed Performance Statistics (Filtered Data):")
    print(f"TTFT - Mean: {np.mean(ttft_values):.4f}s, Std: {np.std(ttft_values):.4f}s")
    print(f"TPOT - Mean: {np.mean(tpot_values):.4f}s, Std: {np.std(tpot_values):.4f}s")
    if len(paired_data) > 1:
        correlation = np.corrcoef(paired_tpot, paired_ttft)[0, 1]
        print(f"TTFT-TPOT Correlation: {correlation:.4f}")
    
    return stats, success_count

async def run_experiment(trace_file, api_url, model_name, num_requests=None, duration_minutes=None, middle_ratio=0.8):
    """
    ì‹¤í—˜ ì‹¤í–‰
    
    Args:
        trace_file: trace íŒŒì¼ ê²½ë¡œ
        api_url: API URL
        model_name: ëª¨ë¸ ì´ë¦„
        num_requests: ì‚¬ìš©í•  ìš”ì²­ ê°œìˆ˜ (legacy)
        duration_minutes: ì‹¤í—˜ ì§€ì† ì‹œê°„ (ë¶„ ë‹¨ìœ„)
        middle_ratio: ì„±ëŠ¥ ë¶„ì„ìš© ì¤‘ê°„ êµ¬ê°„ ë¹„ìœ¨
    """
    print(f"\n--- Running trace-based experiment ---")
    
    # ğŸ†• ì „ì—­ ë³€ìˆ˜ ì´ˆê¸°í™”
    global iteration_details
    iteration_details.clear()
    
    # trace ë°ì´í„° ë¡œë“œ
    trace_df = load_trace_data(trace_file, num_requests=num_requests, duration_minutes=duration_minutes)
    
    # ì‹¤í—˜ ì •ë³´ ì¶œë ¥
    if duration_minutes is not None:
        print(f"Experiment setup: {duration_minutes} minutes, {len(trace_df)} requests")
    elif num_requests is not None:
        print(f"Experiment setup: {num_requests} requests, {trace_df['relative_time'].max()/60:.2f} minutes")
    else:
        print(f"Experiment setup: {len(trace_df)} requests, {trace_df['relative_time'].max()/60:.2f} minutes")
    
    prompts = pregenerate_prompts(trace_df, tokenizer_name=model_name)
    
    experiment_start_time = time.perf_counter()
    print(f"Experiment started at {experiment_start_time:.6f}s")
    results = await execute_trace_based_requests(trace_df, prompts, api_url, model_name)
    experiment_end_time = time.perf_counter()
    print(f"Experiment completed at {experiment_end_time:.6f}s")
    
    # Request ë³„ ê²°ê³¼ ì €ì¥
    results_file = f"benchmark_request.json"
    with open(results_file, 'w') as f:
        json.dump(results, f, indent=2)
    print(f"Results with iteration data saved to {results_file}")
    
    
    # Iteration ë³„ ê²°ê³¼ ì €ì¥.
    processed_iterations = process_iteration_details()
    iteration_file = f"benchmark_iteration.json"
    with open(iteration_file, 'w') as f:
        json.dump(processed_iterations, f, indent=2)
    print(f"Iteration details saved to {iteration_file}")
    
    
    # ğŸ†• iteration í†µê³„ ì¶œë ¥
    print(f"\n--- Iteration Statistics ---")
    print(f"Total iterations executed: {len(processed_iterations)}")
    if processed_iterations:
        total_tokens = sum(iter_data['tokens_generated'] for iter_data in processed_iterations)
        avg_tokens_per_iter = total_tokens / len(processed_iterations)
        print(f"Total tokens generated: {total_tokens}")
        print(f"Average tokens per iteration: {avg_tokens_per_iter:.2f}")
        
        # ìŠ¤ì¼€ì¤„ë§ í†µê³„
        avg_scheduled_reqs = np.mean([iter_data.get('total_scheduled_requests', 0) for iter_data in processed_iterations if iter_data.get('total_scheduled_requests') is not None])
        avg_scheduled_tokens = np.mean([iter_data.get('total_scheduled_tokens', 0) for iter_data in processed_iterations if iter_data.get('total_scheduled_tokens') is not None])
        avg_prefill_reqs = np.mean([iter_data.get('prefill_requests', 0) for iter_data in processed_iterations if iter_data.get('prefill_requests') is not None])
        avg_prefill_tokens = np.mean([iter_data.get('prefill_tokens', 0) for iter_data in processed_iterations if iter_data.get('prefill_tokens') is not None])
        avg_decode_reqs = np.mean([iter_data.get('decode_requests', 0) for iter_data in processed_iterations if iter_data.get('decode_requests') is not None])
        avg_decode_tokens = np.mean([iter_data.get('decode_tokens', 0) for iter_data in processed_iterations if iter_data.get('decode_tokens') is not None])
        
        avg_kv_usage = np.mean([iter_data.get('kv_cache_usage', 0) for iter_data in processed_iterations if iter_data.get('kv_cache_usage') is not None])
        avg_kv_usage_gb = np.mean([iter_data.get('kv_cache_usage_gb', 0) for iter_data in processed_iterations if iter_data.get('kv_cache_usage_gb') is not None])
        avg_kv_total_capacity = np.mean([iter_data.get('kv_cache_total_capacity', 0) for iter_data in processed_iterations if iter_data.get('kv_cache_total_capacity') is not None])
        
        
        print(f"Average scheduled requests per iteration: {avg_scheduled_reqs:.2f}")
        print(f"Average scheduled tokens per iteration: {avg_scheduled_tokens:.2f}")
        print(f"Average prefill requests per iteration: {avg_prefill_reqs:.2f}")
        print(f"Average prefill tokens per iteration: {avg_prefill_tokens:.2f}")
        print(f"Average decode requests per iteration: {avg_decode_reqs:.2f}")
        print(f"Average decode tokens per iteration: {avg_decode_tokens:.2f}")
        
        print(f"Average kv cache usage per iteration: {avg_kv_usage:.4f}"
              f" (GB: {avg_kv_usage_gb:.4f}, Total Capacity: {avg_kv_total_capacity:.4f})")
    
    # iteration ë°ì´í„° ìš”ì•½ ì¶œë ¥ (ê¸°ì¡´)
    total_tokens = sum(len(r['iteration_data']) for r in results if r['success'])
    print(f"Total tokens generated: {total_tokens}")
    print(f"Total Experiment Time: {experiment_end_time - experiment_start_time:.2f}s")
    
    # Store the Total Experiment Time and Total Requests
    with open("benchmark_summary.json", 'w') as f:
        summary = {
            "total_experiment_time": experiment_end_time - experiment_start_time,
            "total_requests": len(results),
            "successful_requests": sum(1 for r in results if r['success']),
            "failed_requests": sum(1 for r in results if not r['success'])
        }
        json.dump(summary, f, indent=2)
    print(f"Summary saved to benchmark_summary.json")
    
    return results

async def main():
    parser = argparse.ArgumentParser(description="vLLM trace-based benchmark client")
    parser.add_argument("--trace-file", 
                       default="./trace/azure_conv_0514_1400_20min_15.0x_tc.csv",
                       help="Path to trace CSV file")
    parser.add_argument("--api-url", 
                       default="http://0.0.0.0:8000/v1/completions", 
                       help="vLLM server API URL")
    parser.add_argument("--model", 
                       default="Qwen/Qwen2.5-7B",
                       help="Model name or path")
    parser.add_argument("--num-requests", 
                       type=int, 
                       default=None,
                       help="Number of requests to use (legacy mode)")
    parser.add_argument("--duration-minutes", 
                       type=float, 
                       default=None,
                       help="Duration of experiment in minutes (preferred over num-requests)")
    parser.add_argument("--middle-ratio", 
                       type=float, 
                       default=0.7,
                       help="Ratio of middle data to use for performance stats (default: 0.5)")
    
    args = parser.parse_args()
    
    # ë‘ íŒŒë¼ë¯¸í„°ê°€ ëª¨ë‘ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’ ì„¤ì •
    if args.num_requests is None and args.duration_minutes is None:
        args.duration_minutes = 20.0  # ê¸°ë³¸ê°’: 20 minutes
        print("No duration or num_requests specified, using default: 20 minutes")
    
    # ë‘ íŒŒë¼ë¯¸í„°ê°€ ëª¨ë‘ ì œê³µëœ ê²½ìš° ì—ëŸ¬
    if args.num_requests is not None and args.duration_minutes is not None:
        print("Error: Cannot specify both --num-requests and --duration-minutes. Please use only one.")
        return
    
    if not args.api_url.endswith(("completions", "profile")):
        if not args.api_url.endswith('/'):
            args.api_url += '/v1/completions'
        else:
            args.api_url += 'v1/completions'
    
    try:
        await run_experiment(
            args.trace_file, 
            args.api_url, 
            args.model, 
            num_requests=args.num_requests,
            duration_minutes=args.duration_minutes,
            middle_ratio=args.middle_ratio
        )
    except ValueError as e:
        print(f"Error: {e}")
        return

if __name__ == "__main__":
    asyncio.run(main())